<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-学习杂记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/09/09/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/" class="article-date">
  <time class="dt-published" datetime="2021-09-09T10:55:38.499Z" itemprop="datePublished">2021-09-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>[toc]</p>
<h1 id="AD算法项目"><a href="#AD算法项目" class="headerlink" title="AD算法项目"></a>AD算法项目</h1><h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><ol>
<li><p>采样率</p>
<blockquote>
<p><strong>采样率</strong>（也称为<strong>采样速度</strong>或者<strong>采样频率</strong>）定义了每秒从<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%BF%9E%E7%BB%AD%E4%BF%A1%E5%8F%B7">连续信号</a>中提取并组成<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%A6%BB%E6%95%A3%E4%BF%A1%E5%8F%B7">离散信号</a>的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E9%87%87%E6%A0%B7">采样</a>个数，它用<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%B5%AB%E5%85%B9">赫兹</a>（Hz）来表示。采样频率的倒数叫作<strong>采样周期</strong>或<strong>采样时间</strong>，它是<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E9%87%87%E6%A0%B7">采样</a>之间的时间间隔。</p>
</blockquote>
</li>
<li><p>时间单位</p>
<p><img src="https://i.loli.net/2021/08/17/E1CNBZMpOivujQ8.png" alt="图片.png"></p>
</li>
<li><p>虚拟串口</p>
</li>
<li><p>波特率</p>
<blockquote>
<p>在电子通信领域，波特（Baud）即调制速率，指的是有效数据讯号调制载波的速率，即单位时间内载波调制状态变化的次数。</p>
<p>波特率表示单位时间内传送的<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E7%A0%81%E5%85%83/10525003">码元</a>符号的个数，它是对符号传输速率的一种度量，它用单位时间内<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%BD%BD%E6%B3%A2/3441949">载波</a>调制状态改变的次数来表示，波特率即指一个单位时间内传输符号的个数。</p>
</blockquote>
</li>
</ol>
<h2 id="Pyserial"><a href="#Pyserial" class="headerlink" title="Pyserial"></a>Pyserial</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/caiya/p/13136785.html">python–serial串口通信</a></p>
<h2 id="PyQt"><a href="#PyQt" class="headerlink" title="PyQt"></a>PyQt</h2><ol>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/jia666666/article/details/81534260">QComboBox</a></p>
</li>
<li><p><a href="">QSpinBox</a> </p>
<p>QSPINBox是一个计数器控件，允许用户选择一个整数值通过单击向上向下或者按键盘上的上下键来增加减少当前显示的值，当然用户也可以输入值</p>
</li>
<li><p>QGridLayout</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/jia666666/article/details/81701176">https://blog.csdn.net/jia666666/article/details/81701176</a></p>
</li>
<li><p>serial.tools.list_ports_windows.comports()</p>
<p>返回计算机上所有的port口信息</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_29666899/article/details/79118404">QtCore.QTimer()</a></p>
<p>当代程序中需要显示时间时或者需要在程序中周期性地进行某项操作，就会用到定时器。PyQt5就提供了一个定时器QTimer来实现这种操作 </p>
</li>
</ol>
<h2 id="信号处理"><a href="#信号处理" class="headerlink" title="信号处理"></a>信号处理</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80625865">信号处理-卷积</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/22298352">如何通俗易懂地解释卷积？</a></p>
<h1 id="深度学习学习杂记"><a href="#深度学习学习杂记" class="headerlink" title="深度学习学习杂记"></a>深度学习学习杂记</h1><h2 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h2><h3 id="Inception块"><a href="#Inception块" class="headerlink" title="Inception块"></a>Inception块</h3><p><img src="https://zh-v2.d2l.ai/_images/inception.svg" alt="../_images![../_images/inception.svg](https://zh-v2.d2l.ai/_images/inception.svg)/inception.svg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># `c1`--`c4` 是每条路径的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3 x 3最大汇聚层后接1 x 1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="残差网络的解释"><a href="#残差网络的解释" class="headerlink" title="残差网络的解释"></a>残差网络的解释</h3><img src="https://i.loli.net/2021/08/11/hp7lGETUAtnQqWc.png" alt="image-20210810154203168.png" style="zoom:67%;" />

<blockquote>
<p>因此，只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。 对于深度神经网络，如果我们能将新添加的层训练成 <em>恒等映射</em>（identity function） $f(x)=x$​ ，新模型和原模型将同样有效。 同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</p>
</blockquote>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40050371">一文读懂卷积神经网络中的1 x 1卷积核</a></p>
<ul>
<li>对于同一通道，卷积权重是共享的</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lanadeus/article/details/82534425">转置卷积</a></p>
<ul>
<li><p>卷积运算可通过一次矩阵运算实现，因此通过矩阵的转置，实现了 $output -&gt; intput $</p>
<p>正向：$kernel_{4\times16} \times input_{16 \times 1} = output_{4 \times 1}$​​</p>
<p>反向：$ (kernel_{4 \times 16})^T \times output_{4 \times 1} = intput_{16 \times 1}$​</p>
</li>
</ul>
<p><strong>深度可分离卷积</strong></p>
<img src="https://i.loli.net/2021/08/11/wvWyHLOr25XejRG.png" alt="image-20210728170901493.png" style="zoom:67%;" />

<h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><ul>
<li><blockquote>
<p>通过使用 <code>torch.jit.script</code> 函数来转换模型，我们就有能力编译和优化多层感知机中的计算，而模型的计算结果保持不变。</p>
</blockquote>
</li>
<li><p>池化层</p>
<ol>
<li><em>池化</em>（pooling）层，它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。</li>
<li>默认情况下，深度学习框架中的步幅与池化窗口的大小相同。</li>
<li>在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 这意味着汇聚层的输出通道数与输入通道数相同。</li>
</ol>
</li>
<li><p>torch.mm 和 torch.mul, torch.matmul的区别</p>
<blockquote>
<ol>
<li><p>torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等，比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵；</p>
</li>
<li><p>torch.mm(a, b)是矩阵a和b矩阵相乘，比如a的维度是(1, 2)，b的维度是(2, 3)，返回的就是(1, 3)的矩阵。<br>PS：更接地气来说区别就是点乘，和矩阵乘法的区别</p>
</li>
<li><p>torch.matmul(input, other, *, out=None) → Tensor</p>
</li>
</ol>
<p>  Matrix product of two tensors.</p>
<p>  The behavior depends on the dimensionality of the tensors as follows:</p>
<p>  <img src="https://i.loli.net/2021/08/11/3CvuUHsFY7LgVjb.png" alt="image-20210811152012438.png"></p>
</blockquote>
</li>
<li><blockquote>
<p>不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy <code>ndarray</code>中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。</p>
</blockquote>
</li>
<li><p>torch.bmm</p>
<p><img src="https://i.loli.net/2021/08/11/JwaKOfFZV73poqj.png" alt="图片.png"></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xiexu911/article/details/80820028">pytorch学习 中 torch.squeeze() 和torch.unsqueeze()的用法</a></p>
</li>
<li><p>torch.repeat_interleave用法</p>
<blockquote>
<p>torch.repeat_interleave(<em>input</em>, <em>repeats</em>, <em>dim=None</em>) → Tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.repeat_interleave(<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.repeat_interleave(y, <span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.repeat_interleave(y, <span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.repeat_interleave(y, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), dim=<span class="number">0</span>)</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure></li>
<li><p>一个模型中出现共享层时，一次backward()是否会导致共享层的最终grad是两个共享层梯度的和（因为一次backward()过程中，共享层第一次计算出的grad没有被清零），从而梯度下降时有更大的步幅，收敛的速度也更快呢？</p>
<blockquote>
<p>理论上是的。你也可以选择关掉（frozen）任何一层的grad。</p>
</blockquote>
</li>
<li><p>参数初始化(使用初始化函数)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.apply(int_function) <span class="comment"># </span></span><br></pre></td></tr></table></figure></li>
<li><p>访问参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br></pre></td></tr></table></figure></li>
<li><p>在定义网络结构时，使用<code>self._modules</code> 和 <code>list</code>的区别</p>
<blockquote>
<p>The main difference is that using _modules enables the other pytorch  functions/methods to find the added layers automatically. To put it in  another word, these layers will be registered. For example, if you want  to print parameters of the network, you can simply call state_dicts().  But if the list is adopted, methods like state_dicts don’t work.<br> Code:<br> class MySequential(nn.Module):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *args</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="keyword">for</span> i,block <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">        self._modules[<span class="built_in">str</span>(i)] = block</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">        X = block(X)</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<p>net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))<br>net.state_dicts()</p>
</blockquote>
</li>
<li><p><code>torch.clamp</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">      | <span class="built_in">min</span>, <span class="keyword">if</span> x_i &lt; <span class="built_in">min</span></span><br><span class="line">y_i = | x_i, <span class="keyword">if</span> <span class="built_in">min</span> &lt;= x_i &lt;= <span class="built_in">max</span></span><br><span class="line">      | <span class="built_in">max</span>, <span class="keyword">if</span> x_i &gt; <span class="built_in">max</span></span><br></pre></td></tr></table></figure>

<ul>
<li>广播机制</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">3</span>).reshape(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">b = np.arange(<span class="number">2</span>).reshape(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">(array([[<span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">2.</span>]]),</span><br><span class="line"> array([[<span class="number">0.</span>, <span class="number">1.</span>]]))</span><br><span class="line">a+b</span><br><span class="line">array([[<span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">       [<span class="number">2.</span>, <span class="number">3.</span>]])</span><br></pre></td></tr></table></figure>

<ul>
<li><p>.numpy<code>和</code>.from_numpy<code>方法是可以共享内存的，但如果使用</code>torch.tensor(ndarray)`由numpy数组生成tensor，就不会共享内存。</p>
</li>
<li><p>os.makedirs(name, mode=0o777, exist_ok=False)</p>
<p>用来创建多层目录</p>
<p>exist_ok：是否在目录存在时触发异常。如果exist_ok为<strong>False</strong>（默认值），则在目标目录已存在的情况下<strong>触发</strong>FileExistsError异常；如果exist_ok为<strong>True</strong>，则在目标目录已存在的情况下<strong>不会触发</strong>FileExistsError异常。</p>
</li>
<li><p>os.path.join()函数：连接两个或更多的路径名组件</p>
<ol>
<li>如果各组件名首字母不包含’/‘，则函数会自动加上</li>
<li>如果有一个组件是一个绝对路径，则在它之前的所有组件均会被舍弃</li>
<li>如果最后一个组件为空，则生成的路径以一个’/‘分隔符结尾</li>
</ol>
</li>
<li><p>```<br>A = torch.arange(20, dtype=torch.float32).reshape(5, 4)<br>B = A.clone()  # 通过分配新内存，将A的一个副本分配给B</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- axis指定哪个维度，哪个维度就会消失，`e.g.`  $5 \times 4$​​ 按照axis=0, 会产生4,的结果</span><br><span class="line"></span><br><span class="line">- ![image-20210728201151785.png](https://i.loli.net/2021/08/11/ZQut5csbeJKdxIN.png)</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  x = np.arange(0, 3, 0.1)</span><br><span class="line">  plot(x, [f(x), 2 * x - 3], &#x27;x&#x27;, &#x27;f(x)&#x27;, legend=[&#x27;f(x)&#x27;, &#x27;Tangent line (x=1)&#x27;])</span><br></pre></td></tr></table></figure></li>
<li><p>PyTorch梯度累加</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://xingwxiong.github.io/2020/04/07/gradients-accumulation/">PyTorch 梯度累积小技巧</a></li>
<li></li>
</ol>
</li>
<li><p><code>random.shuffle</code> () # in-place操作</p>
</li>
<li><p><code>torch.no_grad()</code> 是一个上下文管理器，被该语句 wrap 起来的部分将不会track 梯度。</p>
</li>
<li><p><code>TensorDataset</code> 可以用来对 tensor 进行打包，就好像 python 中的 zip 功能</p>
</li>
<li><p><code>DataLoader</code> 就是用来包装所使用的数据，每次抛出一批数据</p>
</li>
<li><p>训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p><code>Why do we need to call zero_grad() in PyTorch?</code></p>
<p><img src="https://i.loli.net/2021/08/11/GetlfbPrKEyhzk1.png" alt="image-20210802103008159.png"></p>
</li>
<li><p><img src="https://i.loli.net/2021/08/11/vXzr2akjJOcWwA7.png" alt="image-20210802112623723.png"></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.yisu.com/zixun/455005.html">Pytorch中net.train 和 net.eval怎么用</a></p>
</li>
<li><p>torch <code>apply</code> <a target="_blank" rel="noopener" href="https://blog.csdn.net/daydayjump/article/details/80899029">参数初始化</a></p>
</li>
<li><p>重新审视Softmax的实现技巧</p>
<p><img src="https://i.loli.net/2021/08/11/6FS4YEvt1IaDLpn.png" alt="image-20210802154905553.png"></p>
</li>
<li><p>Pytorch的backward()相关理解</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/douhaoexia/article/details/78821428">博客1</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/pengge0433/article/details/79459679">Pytorch中的variable, tensor与numpy相互转化的方法</a></p>
</li>
<li><p>顺序块</p>
</li>
<li><p><img src="https://i.loli.net/2021/08/11/EiWR3JegfbTNP2V.png" alt="image-20210805193118474.png"></p>
</li>
<li><p><code>VGG</code>块的实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure></li>
<li><p>小批量实现多<code>GPU</code>训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">allreduce</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(data)):</span><br><span class="line">        data[<span class="number">0</span>][:] += data[i].to(data[<span class="number">0</span>].device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(data)):</span><br><span class="line">        data[i] = data[<span class="number">0</span>].to(data[i].device)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_batch</span>(<span class="params">X, y, devices</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将`X`和`y`拆分到多个设备上&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> X.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> (nn.parallel.scatter(X, devices),</span><br><span class="line">            nn.parallel.scatter(y, devices))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_batch</span>(<span class="params">X, y, device_params, devices, lr</span>):</span></span><br><span class="line">    X_shards, y_shards = split_batch(X, y, devices)</span><br><span class="line">    <span class="comment"># 在每个GPU上分别计算损失</span></span><br><span class="line">    ls = [loss(lenet(X_shard, device_W), y_shard).<span class="built_in">sum</span>()</span><br><span class="line">          <span class="keyword">for</span> X_shard, y_shard, device_W <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">              X_shards, y_shards, device_params)]</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> ls:  <span class="comment"># 反向传播在每个GPU上分别执行</span></span><br><span class="line">        l.backward()</span><br><span class="line">    <span class="comment"># 将每个GPU的所有梯度相加，并将其广播到所有GPU</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(device_params[<span class="number">0</span>])):</span><br><span class="line">            allreduce([device_params[c][i].grad <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(devices))])</span><br><span class="line">    <span class="comment"># 在每个GPU上分别更新模型参数</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> device_params:</span><br><span class="line">        d2l.sgd(param, lr, X.shape[<span class="number">0</span>]) <span class="comment"># 在这里，我们使用全尺寸的小批量</span></span><br></pre></td></tr></table></figure></li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102697821">Pytorch的nn.DataParallel</a></p>
<p>用多个GPU来加速训练、以及一些疑问</p>
</li>
<li><p><img src="https://i.loli.net/2021/09/08/S7Bi3ZNCKjIdfYW.png" alt="图片.png"></p>
<p><code>nn.CrossEntropyLoss</code></p>
</li>
</ul>
<h2 id="网络的trick"><a href="#网络的trick" class="headerlink" title="网络的trick"></a>网络的trick</h2><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/attention/">Attention 机制</a></p>
<h3 id="批量归一化（BN）"><a href="#批量归一化（BN）" class="headerlink" title="批量归一化（BN）"></a>批量归一化（<code>BN</code>）</h3><blockquote>
<p>批量归一化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先归一化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。 接下来，我们应用比例系数和比例偏移。 正是由于这个基于<em>批量</em>统计的<em>标准化</em>，才有了<em>批量归一化</em>的名称。</p>
<p>请注意，在应用批量归一化时，批量大小的选择可能比没有批量归一化时更重要。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span></span><br><span class="line"><span class="comment"># 这里我们需要保持X的形状以便后面可以做广播运算</span></span><br><span class="line">mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>



<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><blockquote>
<p>此外，你可能会问为什么我们首先使用<code>L2</code>范数，而不是<code>L1</code>范数。事实上，这些选择在整个统计领域中都是有效的和受欢迎的。<code>L2</code>正则化线性模型构成经典的<em>岭回归</em>（ridge regression）算法，<code>L1</code>正则化线性回归是统计学中类似的基本模型，通常被称为<em>套索回归</em>（lasso regression）。</p>
<p>使用<code>L2</code>范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。在实践中，这可能使它们对单个变量中的观测误差更为鲁棒。相比之下，<code>L1</code> 惩罚会导致模型将其他权重清除为零而将权重集中在一小部分特征上。这称为<em>特征选择</em>（feature selection），这可能是其他场景下需要的。</p>
</blockquote>
<h3 id="Drop-out"><a href="#Drop-out" class="headerlink" title="Drop-out"></a>Drop-out</h3><p><img src="https://i.loli.net/2021/08/11/SH2xibAz6g3CoQI.png" alt="image-20210803151851720.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Tensor(X.shape).uniform_(<span class="number">0</span>, <span class="number">1</span>) &gt; dropout).<span class="built_in">float</span>() <span class="comment"># X 应为Tensor</span></span><br></pre></td></tr></table></figure>

<h2 id="Pytorch求解一般性的优化问题"><a href="#Pytorch求解一般性的优化问题" class="headerlink" title="Pytorch求解一般性的优化问题"></a>Pytorch求解一般性的优化问题</h2><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/363852642">用pytorch梯度下降求解非线性规划问题</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/363852642">用pytorch做简单的最优化问题</a></li>
<li><a target="_blank" rel="noopener" href="https://suool.net/2019/03/01/optimization-problem/">https://suool.net/2019/03/01/optimization-problem/</a></li>
</ul>
<h2 id="环境变量设置"><a href="#环境变量设置" class="headerlink" title="环境变量设置"></a>环境变量设置</h2><p><img src="https://i.loli.net/2021/08/11/Z9wGnbjD7P1aJR3.png" alt="image-20210803093618360.png"></p>
<h2 id="相对误差和绝对误差"><a href="#相对误差和绝对误差" class="headerlink" title="相对误差和绝对误差"></a>相对误差和绝对误差</h2><p><img src="https://i.loli.net/2021/08/11/7pMN2IFuOEbGVX5.png" alt="image-20210804195722558.png"></p>
<p><img src="https://i.loli.net/2021/08/11/Bp9neEPQfTvd8gR.png" alt="image-20210804200013859.png"></p>
<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><h3 id="python图像支持中文"><a href="#python图像支持中文" class="headerlink" title="python图像支持中文"></a>python图像支持中文</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 显示中文</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<h3 id="pyplot"><a href="#pyplot" class="headerlink" title="pyplot"></a>pyplot</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.clf() <span class="comment">#清除上一幅图像</span></span><br><span class="line">plt.pause(<span class="number">0.01</span>)  <span class="comment"># 暂停0.01秒</span></span><br><span class="line">plt.ioff()  <span class="comment"># 关闭画图的窗口</span></span><br><span class="line"><span class="comment"># 设置legend</span></span><br><span class="line">line=plt.plot(data)</span><br><span class="line">plt.legend([line],[<span class="string">&#x27;description&#x27;</span>]) <span class="comment"># 若多条曲线以列表格式添加元素</span></span><br><span class="line">plt.scatter(x,y, s=<span class="number">50</span>, c=<span class="string">&#x27;yellow&#x27;</span>) <span class="comment"># 散点图，s控制大小，c控制颜色</span></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_36219858/article/details/79800460">plot参数详解</a></p>
<h3 id="函数的用法"><a href="#函数的用法" class="headerlink" title="函数的用法"></a>函数的用法</h3><ul>
<li><p><strong>slice()</strong> 函数实现切片对象</p>
<p>start –  起始位置，stop –  结束位置，step –  间距</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/walo/p/10608436.html">Python 字符串前面加u,r,b,f的含义</a></p>
</li>
</ul>
<h3 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a>进程和线程</h3><p><a target="_blank" rel="noopener" href="https://www.liaoxuefeng.com/wiki/897692888725344/923056118147584">资料1</a></p>
<h3 id="特殊用法"><a href="#特殊用法" class="headerlink" title="特殊用法"></a>特殊用法</h3><ul>
<li><p>列表前面加星号作用是将列表解开成两个独立的参数，传入函数，</p>
<p>字典前面加两个星号，是将字典解开成独立的元素作为形参。</p>
</li>
</ul>
<h1 id="跟踪算法"><a href="#跟踪算法" class="headerlink" title="跟踪算法"></a>跟踪算法</h1><h3 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h3><ul>
<li></li>
</ul>
<h3 id="DeepSort"><a href="#DeepSort" class="headerlink" title="DeepSort"></a>DeepSort</h3><p><strong>track</strong> 的处理</p>
<ol>
<li>Tracks that exceed a predefined maximum age Amax are considered to have left the scene and are deleted from the track set.</li>
<li>New track hypotheses are initiated for each detection that cannot be associated to an existing track. These new tracks are classified as tentative during their first three frames. During this time, we expect a successful measurement association at each time step. Tracks that are not successfully associated to a measurement within their first three frames are deleted.</li>
</ol>
<p><strong>Assignment Probem</strong></p>
<ol>
<li>On the one hand, the Mahalanobis distance provides information about possible object locations based on motion that are particularly useful for short-term predictions.</li>
<li>On the other hand, the cosine distance considers appearance information that are particularly useful to recover identities after longterm occlusions, when motion is less discriminative.</li>
<li><img src="C:\Users\wanji\AppData\Roaming\Typora\typora-user-images\image-20210813152202512.png" alt="image-20210813152202512" style="zoom:67%;" /></li>
</ol>
<h1 id="外参标定软件"><a href="#外参标定软件" class="headerlink" title="外参标定软件"></a>外参标定软件</h1><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><h4 id="cv2-Rodrigues"><a href="#cv2-Rodrigues" class="headerlink" title="cv2.Rodrigues"></a>cv2.Rodrigues</h4><p><img src="https://i.loli.net/2021/08/11/UyOIDmRsexqLorp.png" alt="image-20210805164958695.png"></p>
<h3 id="QtDesigner"><a href="#QtDesigner" class="headerlink" title="QtDesigner"></a>QtDesigner</h3><h3 id="Python之日志处理（logging模块）"><a href="#Python之日志处理（logging模块）" class="headerlink" title="Python之日志处理（logging模块）"></a><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yyds/p/6901864.html">Python之日志处理（logging模块）</a></h3><h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><h2 id="VsCode下的C-配置"><a href="#VsCode下的C-配置" class="headerlink" title="VsCode下的C++配置"></a>VsCode下的C++配置</h2><ol>
<li><p>VsCode下载c/c++扩展</p>
</li>
<li><p>下载MinGW解释器</p>
</li>
<li><p>将MinGW添加到环境变量中</p>
<p>在终端 输入 <code>gcc -v</code>测试是否成功</p>
</li>
<li><img src="https://i.loli.net/2021/08/13/1S3dxEAzyQBwr8c.gif" alt="vscode_c__设置.gif" style="zoom:67%;" /></li>
</ol>
<p><code>launch.json</code>和<code>tasks.json</code>会自动生成，不需要像某些博客讲的手动填入内容</p>
<h1 id="C-学习笔记"><a href="#C-学习笔记" class="headerlink" title="C++学习笔记"></a>C++学习笔记</h1><h2 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h2><h3 id="auto"><a href="#auto" class="headerlink" title="auto"></a>auto</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/QG-whz/p/4951177.html">C++11特性：auto关键字</a></p>
<h3 id="extern用法"><a href="#extern用法" class="headerlink" title="extern用法"></a><strong>extern用法</strong></h3><blockquote>
<p><strong>extern</strong>是一种“<strong>外部声明</strong>”的关键字，字面意思就是<strong>在此处声明</strong>某种变量或函数，<strong>在外部定义</strong>。</p>
</blockquote>
<h3 id="C-中的-inline-用法"><a href="#C-中的-inline-用法" class="headerlink" title="C++ 中的 inline 用法"></a>C++ 中的 inline 用法</h3><blockquote>
<p>在 c/c++ 中，为了解决一些频繁调用的小函数大量消耗栈空间（栈内存）的问题，特别的引入了 inline 修饰符，表示为内联函数。</p>
<p>栈空间就是指放置程序的局部数据（也就是函数内数据）的内存空间。</p>
<p><a target="_blank" rel="noopener" href="https://www.runoob.com/w3cnote/cpp-inline-usage.html">https://www.runoob.com/w3cnote/cpp-inline-usage.html</a></p>
</blockquote>
<h3 id="this指针"><a href="#this指针" class="headerlink" title="this指针"></a>this指针</h3><blockquote>
<p>在 C++ 中，每一个对象都能通过 <strong>this</strong> 指针来访问自己的地址。<strong>this</strong> 指针是所有成员函数的隐含参数。因此，在成员函数内部，它可以用来指向调用对象。</p>
<p>友元函数没有 <strong>this</strong> 指针，因为友元不是类的成员。只有成员函数才有 <strong>this</strong> 指针。</p>
</blockquote>
<h2 id="注意的点"><a href="#注意的点" class="headerlink" title="注意的点"></a>注意的点</h2><ol>
<li>一个程序局部和全局变量的名称可以相同，但局部变量的值在函数内部将优先采用。</li>
<li>当局变量被定义，它不是由系统初始化，而是用户必须自己初始化。全局变量是由当它们定义为如下系统自动初始化.</li>
<li>引用通常用于函数参数列表和函数返回值。</li>
<li>通过小实例，我们无法区分 cout、cerr 和 clog 的差异，但在编写和执行大型程序时，它们之间的差异就变得非常明显。所以良好的编程实践告诉我们，使用 cerr 流来显示错误消息，而其他的日志消息则使用 clog 流来输出。</li>
<li><img src="https://i.loli.net/2021/08/16/NFazgkWo1i2Bj35.png" alt="图片.png" style="zoom:80%;" /></li>
<li><img src="https://i.loli.net/2021/08/16/ZzecHC14xr7J8WR.png" alt="图片.png" style="zoom:80%;" /></li>
</ol>
<h2 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h2><ol>
<li>C++ 字符串指针和字符串指针数组详解](<a target="_blank" rel="noopener" href="https://www.huaweicloud.com/articles/12586791.html">https://www.huaweicloud.com/articles/12586791.html</a>)</li>
<li><a target="_blank" rel="noopener" href="http://www.baidu.com/link?url=nHwC56N9Dl_iJKoAQ9uIxi2ayT25E0kLbS9p2lB9AaQ0Qm06lz2CIG-7SnBsVPjNOZGqaxZnbnHBweSprU6Dma&wd=&eqid=fcddb67f00049f4d00000003611e1e33">C++中NULL和nullptr的区别 - 苦涩的茶 - 博客园</a></li>
</ol>
<h2 id="零散知识点"><a href="#零散知识点" class="headerlink" title="零散知识点"></a>零散知识点</h2><ol>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/huachizi/article/details/89518453">warning: ISO C++ forbids converting a string constant to ‘char*’ [-Wwrite-strings]</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21997625/article/details/84672775">C++map和unordered_map区别和使用</a></p>
<p><a target="_blank" rel="noopener" href="http://c.biancheng.net/view/7231.html">C++ STL unordered_map容器用法详解</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq457163027/article/details/54237782">C++ 点(.)操作符和箭头(-&gt;)操作符</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://c.biancheng.net/cpp/biancheng/view/136.html">c++中的模板函数</a></p>
</li>
</ol>
<h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><h3 id="vector"><a href="#vector" class="headerlink" title="vector"></a>vector</h3><p><a target="_blank" rel="noopener" href="https://www.runoob.com/w3cnote/cpp-vector-container-analysis.html">C++ vector 容器浅析</a></p>
<p><img src="C:\Users\wanji\AppData\Roaming\Typora\typora-user-images\image-20210906172718775.png" alt="image-20210906172718775"></p>
<h3 id="stack"><a href="#stack" class="headerlink" title="stack"></a><a target="_blank" rel="noopener" href="http://c.biancheng.net/view/6971.html">stack</a></h3><h3 id="stack-及-map-的用法示例"><a href="#stack-及-map-的用法示例" class="headerlink" title="stack 及 map 的用法示例"></a>stack 及 map 的用法示例</h3><p><strong>判断有效的括号字符串</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isValid</span><span class="params">(string s)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n=s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (n%<span class="number">2</span>==<span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 字典和栈的定义</span></span><br><span class="line">        unordered_map&lt;<span class="keyword">char</span>,<span class="keyword">char</span>&gt; pairs=&#123;</span><br><span class="line">            &#123;<span class="string">&#x27;)&#x27;</span>, <span class="string">&#x27;(&#x27;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;]&#x27;</span>, <span class="string">&#x27;[&#x27;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;&#125;&#x27;</span>, <span class="string">&#x27;&#123;&#x27;</span>&#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        stack&lt;<span class="keyword">char</span>&gt; stk;</span><br><span class="line">        <span class="comment">// 循环方式</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">char</span> ch:s)&#123;</span><br><span class="line">            <span class="comment">// 判断 键&quot;ch&quot;是否在 map中</span></span><br><span class="line">            <span class="keyword">if</span> (pairs.<span class="built_in">count</span>(ch))&#123;</span><br><span class="line">                <span class="comment">// 判断栈是否为空以及判断栈顶元素是否等于键&quot;ch&quot;的值</span></span><br><span class="line">                <span class="keyword">if</span> (stk.<span class="built_in">empty</span>() || stk.<span class="built_in">top</span>()!=pairs[ch])</span><br><span class="line">                &#123;<span class="keyword">return</span> <span class="literal">false</span>;&#125;</span><br><span class="line">                stk.<span class="built_in">pop</span>(); <span class="comment">// 弹出元素</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                stk.<span class="built_in">push</span>(ch);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> stk.<span class="built_in">empty</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h2 id="类"><a href="#类" class="headerlink" title="类"></a>类</h2><h3 id="unordered-map"><a href="#unordered-map" class="headerlink" title="unordered_map"></a>unordered_map</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> x:unomap)<span class="comment">//遍历整个map，输出key及其对应的value值</span></span><br><span class="line">&#123;</span><br><span class="line">	x.second = <span class="number">0</span>;	</span><br><span class="line">	cout&lt;&lt;x.second&lt;&lt;endl;<span class="comment">//全是  000；;	</span></span><br><span class="line">&#125;</span><br><span class="line">cout&lt;&lt;x.second&lt;&lt;endl;<span class="comment">//回复原来的数值的。</span></span><br><span class="line"><span class="comment">//彻底改变：使用find彻底找到这个数值，然后在进行改，可以保证作用域是整个程序。</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> x:unomap)<span class="comment">//遍历整个map，输出key及其对应的value值</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="keyword">auto</span> it = umap.<span class="built_in">find</span>(key) <span class="comment">//改</span></span><br><span class="line">	<span class="keyword">if</span>(it != umap.<span class="built_in">end</span>()) </span><br><span class="line">	    it-&gt;second = new_value; </span><br><span class="line">&#125;	</span><br></pre></td></tr></table></figure>

<p><strong>注意：使用auto循环时候，修改的值作用域仅仅循环之内，出去循环还会变成未修改的数值。</strong></p>
<h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p><img src="https://i.loli.net/2021/08/17/WKSz68CqZJtXpLh.png" alt="图片.png"></p>
<h3 id="友元函数"><a href="#友元函数" class="headerlink" title="友元函数"></a>友元函数</h3><p>类的友元函数是定义在类外部，但有权访问类的所有私有（private）成员和保护（protected）成员。尽管友元函数的原型有在类的定义中出现过，但是友元函数并不是成员函数。</p>
<p>友元可以是一个函数，该函数被称为友元函数；友元也可以是一个类，该类被称为友元类，在这种情况下，整个类及其所有成员都是友元。</p>
<p>如果要声明函数为一个类的友元，需要在类定义中该函数原型前使用关键字 <strong>friend</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Box</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="keyword">double</span> width;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   <span class="keyword">double</span> length;</span><br><span class="line">   <span class="function"><span class="keyword">friend</span> <span class="keyword">void</span> <span class="title">printWidth</span><span class="params">( Box box )</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">setWidth</span><span class="params">( <span class="keyword">double</span> wid )</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>




      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/09/09/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/" data-id="cktctemik0000xcenbogh4beb" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/09/09/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>